{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.5700, 5.5900, 5.8200, 8.1900, 5.6300, 4.8900, 3.3900, 2.1800, 4.8400,\n",
      "        6.0400, 6.8400])\n"
     ]
    }
   ],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]  # target label \n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]  # input data \n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)\n",
    "\n",
    "t_un = 0.1 * t_u    # uniformed input data between[-10.0, 10.0]\n",
    "print(t_un)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design your model \n",
    "* model define \n",
    "* initialize the parameters and instantiate your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u:torch.Tensor, w:torch.Tensor, b:torch.Tensor):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _Start: initialize the parameters \n",
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n",
    "# _End: initialize the parameters \n",
    "\n",
    "\n",
    "# _Start: instantiate a model with initial values \n",
    "t_p = model(t_u, w, b)\n",
    "# _End: instantiate a model with initial values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function : MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p:torch.Tensor, t_c:torch.Tensor) -> torch.Tensor:  # MSE \n",
    "    squared_diffs = (t_p - t_c) ** 2 \n",
    "    return squared_diffs.mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Autograd \n",
    "* set ```requires_grad = True ``` for using <b>Autograd</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial PyTorch tensors' 'grad' is None:  True\n"
     ]
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.1], requires_grad=True)\n",
    "\n",
    "print(\"Initial PyTorch tensors' \\'grad\\' is None: \", params.grad is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derivatives of the loss with respect to each element of params:  tensor([4527.6567,   82.8000])\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn( model(t_u, *params), t_c)\n",
    "\n",
    "loss.backward() # backpropagation for derivative \n",
    "\n",
    "print(\"derivatives of the loss with respect to each element of params: \",params.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set zero gradient \n",
    "* Before computing the gradients, it should be set zero \n",
    "    > Use ```.zero_()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derivatives of the loss with respect to each element of params:  tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "if params.grad is not None: \n",
    "    params.grad.zero_() \n",
    "    \n",
    "print(\"derivatives of the loss with respect to each element of params: \",params.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs:int, learning_rate:float , params:torch.Tensor, t_u:torch.Tensor, t_c:torch.Tensor) -> torch.Tensor:\n",
    "    \n",
    "    # _Start: training loop for n_poches\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        # _Start: set zero gradient\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "        # _End: set zero gradient\n",
    "\n",
    "        \n",
    "        # _Start: model prediction and estimate the loss \n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        # _End: model prediction and estimate the loss \n",
    "        \n",
    "        \n",
    "        # _Start: backpropagate and update the parameters\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "        # _End: backpropagate and update the parameters\n",
    "            \n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    \n",
    "    # _End: training loop for n_poches\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860116\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957697\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0], requires_grad=True),\n",
    "    t_u = t_un,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. training loop with optimizers \n",
    "* setting hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'Optimizer',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'lr_scheduler']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim \n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "t_p = model(t_un, *params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer's working test \n",
    "* ```optimizer.zero_grad()```\n",
    "* ```loss.backward()```\n",
    "* ```optimizer.step()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before update:  tensor([1., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before update: \", params )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After update:  tensor([1.7761, 0.1064], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "optimizer.zero_grad() \n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"After update: \", params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, Let's define the new training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_with_optim(n_epochs:int, params:torch.Tensor, t_u:torch.Tensor, t_c: torch.Tensor, optimizer) -> torch.Tensor:\n",
    "    \n",
    "    # _Start: training loop for n_poches\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "\n",
    "    # _End: training loop for n_poches\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860116\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957697\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2 \n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "\n",
    "training_loop_with_optim(\n",
    "    n_epochs = 5000,\n",
    "    params = params,\n",
    "    t_u = t_un,\n",
    "    t_c = t_c,\n",
    "    optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. training loop with validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11])\n",
      "11\n",
      "the total number of samples:  11\n"
     ]
    }
   ],
   "source": [
    "print(t_u.shape)\n",
    "print(t_u.shape[0])\n",
    "\n",
    "n_samples = t_u.shape[0]\n",
    "print(\"the total number of samples: \", n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of validation:  2\n"
     ]
    }
   ],
   "source": [
    "n_val = int(0.2 * n_samples)\n",
    "print(\"the number of validation: \", n_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7,  5,  4,  1,  6,  8,  9,  0,  2,  3, 10])\n"
     ]
    }
   ],
   "source": [
    "shuffled_indices = torch.randperm(n_samples)\n",
    "print(shuffled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u = t_u[train_indices]   # input for train \n",
    "train_t_c = t_c[train_indices]   # label for train \n",
    "\n",
    "val_t_u = t_u[val_indices]       # input for validation \n",
    "val_t_c = t_c[val_indices]       # label for validation \n",
    "\n",
    "train_t_un = 0.1 * train_t_u     # uniformed \n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_with_validation(n_epochs:int, optimizer, params:torch.Tensor, train_t_u, val_t_u, train_t_c, val_t_c):\n",
    "    \n",
    "    # _Start: training loop for n_poches\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        # _Start: for training set \n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "        # _End: for training set \n",
    "        \n",
    "        # _Start: for validation set \n",
    "        val_t_p = model(val_t_u, *params)\n",
    "        val_loss = loss_fn(val_t_p, val_t_c)\n",
    "        # End: for validation set \n",
    "    \n",
    "        # _Start: backpropagation and update \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        # _End: backpropagation and update \n",
    "\n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "            print('Epoch {}, Training loss {}, Validation loss {}'.format(\n",
    "                epoch, float(train_loss), float(val_loss)))\n",
    "    \n",
    "    # _End: training loop for n_poches\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 32.34067916870117, Validation loss 296.4708251953125\n",
      "Epoch 2, Training loss 21.572343826293945, Validation loss 206.237548828125\n",
      "Epoch 3, Training loss 18.682376861572266, Validation loss 166.02215576171875\n",
      "Epoch 500, Training loss 6.659817695617676, Validation loss 57.58177185058594\n",
      "Epoch 1000, Training loss 3.6122732162475586, Validation loss 31.565763473510742\n",
      "Epoch 1500, Training loss 2.776271104812622, Validation loss 21.065752029418945\n",
      "Epoch 2000, Training loss 2.5469415187835693, Validation loss 16.423871994018555\n",
      "Epoch 2500, Training loss 2.4840309619903564, Validation loss 14.227890014648438\n",
      "Epoch 3000, Training loss 2.4667739868164062, Validation loss 13.142255783081055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  4.7232, -14.6063], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop_with_validation(\n",
    "    n_epochs = 3000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    train_t_u = train_t_un,\n",
    "    val_t_u = val_t_un,\n",
    "    train_t_c = train_t_c,\n",
    "    val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit: autograd for validation is no needed \n",
    "* Actually, we don't need to backpropagate for validation set \n",
    "* So, switch off autograd for validation. \n",
    "    > for escaping accumulated gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_edit(n_epochs:int, optimizer, params:torch.Tensor, train_t_u, val_t_u, train_t_c, val_t_c):\n",
    "    \n",
    "    # _Start: training loop for n_poches\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        # _Start: for training set \n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "        # _End: for training set\n",
    "        \n",
    "        # _Start: for validation set \n",
    "        with torch.no_grad():\n",
    "            val_t_p = model(val_t_u, *params)\n",
    "            val_loss = loss_fn(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False\n",
    "        # _End: for validation set \n",
    "            \n",
    "        # _Start: backpropagation and update \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        # _End: backpropagation and update \n",
    "        \n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "            print('Epoch {}, Training loss {}, Validation loss {}'.format(\n",
    "                epoch, float(train_loss), float(val_loss)))\n",
    "    \n",
    "    # _End: training loop for n_poches\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 2.4667561054229736, Validation loss 13.140748977661133\n",
      "Epoch 2, Training loss 2.466740369796753, Validation loss 13.139253616333008\n",
      "Epoch 3, Training loss 2.4667227268218994, Validation loss 13.137746810913086\n",
      "Epoch 500, Training loss 2.4620394706726074, Validation loss 12.591374397277832\n",
      "Epoch 1000, Training loss 2.460740804672241, Validation loss 12.30767822265625\n",
      "Epoch 1500, Training loss 2.4603841304779053, Validation loss 12.160470962524414\n",
      "Epoch 2000, Training loss 2.4602880477905273, Validation loss 12.083722114562988\n",
      "Epoch 2500, Training loss 2.4602601528167725, Validation loss 12.043630599975586\n",
      "Epoch 3000, Training loss 2.460251808166504, Validation loss 12.022647857666016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  4.7843, -14.9109], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop_edit(\n",
    "    n_epochs = 3000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    train_t_u = train_t_un,\n",
    "    val_t_u = val_t_un,\n",
    "    train_t_c = train_t_c,\n",
    "    val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
