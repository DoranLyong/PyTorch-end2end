{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font style=\"color:blue\">Instance Segmentation using Mask RCNN</font>\n",
    "\n",
    "In this session, we will learn how to train instance segmentation model using detectron2's pretrained Mask RCNN model. First, lets understand instance segmentation. Instance segmentation task is the labelling of each foreground pixel with object and instance.\n",
    "\n",
    "**Instant Segmentation -> Object Detection + Semantic Segmentation**\n",
    "\n",
    "Instance Segmentation is challenging because it requires the correct detection of all objects in an image while also precisely segmenting each instance. It therefore combines elements from the classical computer vision tasks of object detection, where the goal is to classify individual objects and localize each using a bounding box, and semantic segmentation, where the goal is to classify each pixel into a fixed set of categories without differentiating object instances.\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w11-instance-egmentation.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font style=\"color:green\">1. Model</font>\n",
    "\n",
    "Now, lets understand how Mask RCNN performs instance segmentation. **Mask RCNN** extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition.\n",
    "\n",
    "In brief, **Faster RCNN**  consists of two stages. The first stage, called a Region Proposal Network (RPN),\n",
    "proposes candidate object bounding boxes. The second stage, which is in essence Fast R-CNN, extracts features using RoIPool from each candidate box and performs classification and bounding-box regression. Mask R-CNN adopts the same two-stage procedure, with an identical first stage (which is RPN). In the second stage, in parallel to predicting the class and box offset, Mask R-CNN also outputs a binary mask for each RoI.\n",
    "\n",
    "Lets see how we can train custom object using detectron2 and pretrained Mask RCNN model. We will see how **backpacks** can be detected and segmented in videos using our trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:green\">2. Dataset Ingestion</font>\n",
    "\n",
    "We will use OpenImages dataset to download backpack images, masks and their corresponding labels. The detectron2's standard dataset format is COCO format so here, we will show how to convert the openImages dataset to COCO format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.1. Introduction to OpenImages</font>\n",
    "\n",
    "\n",
    "**[Go to OpenImages](https://storage.googleapis.com/openimages/web/index.html)**\n",
    "\n",
    "Google has introduced OpenImages, an open source dataset containing `~ 9 million` images annotated with labels spanning thousands of object categories. In `2018`, it has released OpenImages `V4` which included `15.4M` bounding-boxes for `600` object categories, making it the largest existing dataset with object location annotations, as well as over `300k` visual relationship annotations. Recently, it has released Open Images `V5`, which adds segmentation masks to the set of annotations.\n",
    "\n",
    "Open Images `V5` features segmentation masks for `2.8 million` object instances in `350` categories. Unlike bounding-boxes, which only identify regions in which an object is located, segmentation masks mark the outline of objects, characterizing their spatial extent to a much higher level of detail. Importantly, these masks cover a broader range of object categories and a larger total number of instances than any previous dataset.\n",
    "\n",
    "**For instance segmentation, COCO format will have json containing all the images, segmentations information. So, we will also show how this json can be created.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.2. Download Mask Images</font>\n",
    "\n",
    "**There are two methods to prepare mask images:**\n",
    "\n",
    "- (a) Download the prepared mask images for the backpack class.\n",
    "\n",
    "\n",
    "- (b) Download the class description file, and all mask images then filter mask images for the backpack.\n",
    "\n",
    "Although the first method is the easiest one, however, it is recommended to go through the second method as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">2.2.a. Prepared Mask Images for Backpacks</font>\n",
    "\n",
    "**You can download prepared masks data from this [link](https://www.dropbox.com/s/uj57mbztvyg5tiq/backpack-masks.zip?dl=1) and unzip it.**\n",
    "\n",
    "\n",
    "**You can also download and unzip data by running cells.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the root directory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# create a data root\n",
    "data_root = '../../data'\n",
    "\n",
    "os.makedirs(data_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to download using URL.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "def download(url, filepath):\n",
    "    \n",
    "    response = urllib.request.urlretrieve(url, filepath)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's declare download_unzip flag, make it true when we have to download the data else make it false.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it true to download all required data\n",
    "download_unzip = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_backpacks_url = 'https://www.dropbox.com/s/uj57mbztvyg5tiq/backpack-masks.zip?dl=1'\n",
    "mask_backpacks_zip_path = os.path.join(data_root, 'backpack-masks.zip')\n",
    "\n",
    "if download_unzip:\n",
    "    download(mask_backpacks_url, mask_backpacks_zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for unzips a zip-file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip(zip_filepath, target_dir):\n",
    "    with zipfile.ZipFile(zip_filepath,'r') as zip_file:\n",
    "        zip_file.extractall(target_dir)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "backpacks_unzip_path = data_root\n",
    "\n",
    "if download_unzip:\n",
    "    unzip(mask_backpacks_zip_path, backpacks_unzip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">2.2.b. Download and Filter</font>\n",
    "\n",
    "We have to download mask images for backpacks. First, we need to find whether the mask for **backpacks** is available or not. If it is available, what is its ClassId? To check this, we need to download `class-descriptions-segmentable.csv`.\n",
    "\n",
    "**[Download class-descriptions-segmentable.csv](https://www.dropbox.com/s/wtl8s3ofy45ezgl/class-descriptions-segmentable.csv?dl=1)**\n",
    "\n",
    "\n",
    "**You can also download by running the below cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_url = 'https://www.dropbox.com/s/wtl8s3ofy45ezgl/class-descriptions-segmentable.csv?dl=1'\n",
    "class_descriptions_file_path = os.path.join(data_root, 'class-descriptions-segmentable.csv')\n",
    "\n",
    "if download_unzip:\n",
    "    download(csv_url, class_descriptions_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's find classId for backpacks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_class_id(classname, csv_path='class-descriptions-segmentable.csv'):\n",
    "    with open(csv_path, 'r') as f:\n",
    "        for line in f:\n",
    "            clss = line.rstrip().split(\",\")\n",
    "\n",
    "            if clss[1] == classname:\n",
    "                return clss[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class ID of Backpack: /m/01940j\n"
     ]
    }
   ],
   "source": [
    "class_name = 'Backpack'\n",
    "\n",
    "backpack_class_id = find_class_id(class_name, class_descriptions_file_path)\n",
    "print('Class ID of Backpack: {}'.format(backpack_class_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above file, we can select any class of our choice and note the classid to get the images and masks belonging to the selected class. Here, the selected class **Backpack** classID is **`m01940j`**.\n",
    "\n",
    "We will download mask images from segmentation train image folders of **OpenImages V6**. Then collect the backpack masks from all the folders into single folder.\n",
    "\n",
    "#### Download Links\n",
    "https://storage.googleapis.com/openimages/v5/train-masks/train-masks-0.zip\n",
    "\n",
    "https://storage.googleapis.com/openimages/v5/train-masks/train-masks-1.zip\n",
    "\n",
    "https://storage.googleapis.com/openimages/v5/train-masks/train-masks-2.zip\n",
    "\n",
    "https://storage.googleapis.com/openimages/v5/train-masks/train-masks-3.zip\n",
    "\n",
    "\n",
    "We can download the raw mask data from the above links and unzip it.\n",
    "\n",
    "You can also download and unzip data by running the below code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download `train-masks-*.zip` and `unzip` it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_url = 'https://storage.googleapis.com/openimages/v5/train-masks/train-masks-{}.zip'\n",
    "mask_filepath = os.path.join(data_root, 'train-masks-{}.zip')\n",
    "\n",
    "unzip_path = os.path.join(data_root, 'train-masks-{}')\n",
    "\n",
    "if download_unzip:\n",
    "    for i in range(4):\n",
    "        res = download(mask_url.format(i), mask_filepath.format(i))\n",
    "        print(res)\n",
    "        unzip(mask_filepath.format(i), unzip_path.format(i))\n",
    "        print('File {} unzipped to {}'.format(mask_filepath.format(i), unzip_path.format(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter backpacks mask images\n",
    "\n",
    "We can also download more directory similar to above. Once the mask folders are downloaded, lets select the backpack masks among all the mask folders and store it in a separate folder `backpack-masks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def copy_mask_for_classid(classid, source_dir, target_dir):\n",
    "    \n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "        \n",
    "    for img_file in os.listdir(source_dir):\n",
    "        if classid in img_file:\n",
    "            file_path = os.path.join(source_dir, img_file)\n",
    "            shutil.copy(file_path, target_dir)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "backpack_mask_dir = os.path.join(data_root, 'backpack-masks')\n",
    "\n",
    "if download_unzip:\n",
    "    for i in range(4):\n",
    "        copy_mask_for_classid(backpack_class_id.replace('/', ''), unzip_path.format(i), backpack_mask_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.3. Download Annotations File</font>\n",
    "\n",
    "We will have to download the annotation file for segmentation.\n",
    "\n",
    "**[Download the annotation file](https://storage.googleapis.com/openimages/v5/train-annotations-object-segmentation.csv).**\n",
    "\n",
    "You can also download by executing the below cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seg_url = 'https://storage.googleapis.com/openimages/v5/train-annotations-object-segmentation.csv'\n",
    "train_seg_csv_path = os.path.join(data_root, 'train-annotations-object-segmentation.csv')\n",
    "\n",
    "if download_unzip:\n",
    "    download(train_seg_url, train_seg_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2.4. Download Images</font>\n",
    "\n",
    "We have to download only those images that have masks and annotations. \n",
    "\n",
    "The mask image files path are in the format of `imageID_classID_maskID.png`. Using the `imageIDs`, we will download the images using `s3`.\n",
    "\n",
    "**Let's write a function to get `imageId` form the mask filename.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imageid_from_masks(mask_dir_path):\n",
    "    \n",
    "    ids_list = os.listdir(mask_dir_path)\n",
    "    img_ids = []\n",
    "\n",
    "    for idl in ids_list:\n",
    "        t = idl.split(\"_\")[0]\n",
    "        img_ids.append(t)\n",
    "\n",
    "    return set(img_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's write a function to get class annotations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def get_class_annotations(classes_list, class_descriptions_csv, annotations_csv):\n",
    "    #list of selected classes\n",
    "    classes = classes_list\n",
    "\n",
    "    #Create dict with className to classId mapping\n",
    "    with open(class_descriptions_csv, mode='r') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        dict_list = {rows[1]:rows[0] for rows in reader}\n",
    "    \n",
    "    csvreader = open(annotations_csv, \"r\")\n",
    "    class_annotations = {}\n",
    "    \n",
    "    for ind in range(0, len(classes)):\n",
    "        class_name = classes[ind]\n",
    "        class_annotations[class_name] = []\n",
    "        print('Class {}: {}'.format(ind, class_name))\n",
    "\n",
    "        ##Select the annotations with backpack classID\n",
    "        for line in csvreader:\n",
    "            if dict_list[class_name] in line:\n",
    "                class_annotations[class_name].append(line.rstrip())\n",
    "                \n",
    "    return class_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a function to get image Ids and annotations. But we do not have images. Let's write a function to download images using image Ids and annotations.\n",
    "\n",
    "We have to download images from S3-bucket (AWS storage). Downloading files from s3 requires an AWS-CLI app or boto3 python package.  In the function, we will use the AWS-CLI app. \n",
    "\n",
    "### Check if AWS already Installed\n",
    "\n",
    "Run the following command in **`CLI`** to check if AWS already installed: \n",
    "```\n",
    "aws --version\n",
    "```\n",
    "Running this command should have the output similar to the following:\n",
    "```\n",
    "aws-cli/2.0.13 Python/3.7.3 Linux/5.3.0-51-generic botocore/2.0.0dev17\n",
    "```\n",
    "\n",
    "If not, you have to install AWS:\n",
    "\n",
    "- [Install AWS CLI MacOS](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-mac.html)\n",
    "\n",
    "\n",
    "- [Install AWS CLI Linux](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html)\n",
    "\n",
    "\n",
    "- [Install AWS CLI Windows](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-windows.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def download_images_for_a_class(annotations, class_name, mask_dir_path, download_dir):\n",
    "    #Create directory for downloading images\n",
    "\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "        \n",
    "    class_annotations = annotations[class_name]\n",
    "    total_annotations = len(class_annotations)\n",
    "    image_id_set = get_imageid_from_masks(mask_dir_path)\n",
    "    print('Number of unique IDs: {}'.format(len(image_id_set)))\n",
    "\n",
    "    for line in class_annotations[0:total_annotations]:\n",
    "        line_parts = line.split(',')\n",
    "        image_id = line_parts[1]\n",
    "        \n",
    "\n",
    "        if image_id not in image_id_set:\n",
    "            continue\n",
    "            \n",
    "        print(image_id)    \n",
    "        image_url = 's3://open-images-dataset/train/{}.jpg'.format(image_id)\n",
    "        print(image_url)\n",
    "        download_path = os.path.join(download_dir, '{}.jpg'.format(image_id))\n",
    "        subprocess.run(['aws', 's3', '--no-sign-request', '--only-show-errors', 'cp', image_url, download_path])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `train-annotations-object-segmentation.csv`, all the backpack records are collected. These records are used for downloading the images using imageID from s3-bucket and also for grouping all the segmentation boxes belonging to the same image based on ImageID key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Backpack\n",
      "Total number of annotations: 1082\n"
     ]
    }
   ],
   "source": [
    "annotations = get_class_annotations(['Backpack'], class_descriptions_file_path, train_seg_csv_path)\n",
    "\n",
    "print('Total number of annotations: {}'.format(len(annotations['Backpack'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "backpack_image_dir = os.path.join(data_root, 'backpack-images')\n",
    "\n",
    "if download_unzip:\n",
    "    download_images_for_a_class(annotations, 'Backpack', backpack_mask_dir, backpack_image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:green\">3. Data Preparation for Training</font>\n",
    "\n",
    "- We have a total of 329 training images. Data should be split into the train and validation set.\n",
    "\n",
    "\n",
    "- We need data in the COCO JSON format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3.1. Split Data into Train and Validation</font>\n",
    "\n",
    "**Let's write a function to split data into train and validation in a `9:1` ratio.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_validation(data_dir_path):\n",
    "    \n",
    "    img_filenames = os.listdir(data_dir_path)\n",
    "    train = []\n",
    "    val = []\n",
    "    for i, img_file in enumerate(img_filenames):\n",
    "        if i%10 == 0:\n",
    "            val.append(img_file)\n",
    "        else:\n",
    "            train.append(img_file)\n",
    "            \n",
    "    print('Number of training samples: {}'.format(len(train)))\n",
    "    print('Number of validation samples: {}'.format(len(val)))\n",
    "    \n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 296\n",
      "Number of validation samples: 33\n"
     ]
    }
   ],
   "source": [
    "train_images, val_images = split_train_validation(backpack_image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3.2. Generate COCO formatted JSON</font>\n",
    "\n",
    "**Sample of COCO JSON format:**\n",
    "\n",
    "```\n",
    "{\n",
    "    \"images\": {\"file_name\": imageID + \".jpg\",\n",
    "               \"height\": imageHeight,\n",
    "               \"width\": imageWidth,\n",
    "               \"id\": imageID\n",
    "              },\n",
    "    \"categories\": { \"supercategory\": \"object\",\n",
    "                    \"id\": classID,\n",
    "                    \"name\": className\n",
    "                  },\n",
    "    \"annotations\": { \"segmentation\": contourPts,\n",
    "                     \"iscrowd\": 0,\n",
    "                     \"image_id\": imageID,\n",
    "                     \"bbox\": bounding box coordinates(XYWH),\n",
    "                     \"category_id\": classID,\n",
    "                     \"id\": boxID\n",
    "                   }\n",
    "}\n",
    "```\n",
    "\n",
    "We will get `contourPts` form mask images and `bounding box coordinates(XYWH)` from annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_all_annotations_for_each_image(annotations, mask_dir_path, class_name, class_index):\n",
    "    \n",
    "    class_annotations = annotations[class_name]\n",
    "    annotations_count = len(class_annotations)\n",
    "    image_id_set = get_imageid_from_masks(mask_dir_path)\n",
    "    \n",
    "    #OpenImages Annotation Format\n",
    "    #MaskPath,ImageID,LabelName,BoxID,BoxXMin,BoxXMax,BoxYMin,BoxYMax,PredictedIoU,Clicks\n",
    "\n",
    "    boxes = defaultdict(list)\n",
    "\n",
    "    for line in class_annotations[0:annotations_count]:\n",
    "        line_parts = line.split(',')\n",
    "\n",
    "        ##Select annotations from those images whose masks we have downloaded\n",
    "        if line_parts[1] not in image_id_set:\n",
    "            continue\n",
    "\n",
    "        mask_path = line_parts[0]\n",
    "        image_id = line_parts[1]\n",
    "\n",
    "        xmin = line_parts[4]\n",
    "        xmax = line_parts[5]\n",
    "        ymin = line_parts[6]\n",
    "        ymax = line_parts[7]\n",
    "        bbox_id = line_parts[3]\n",
    "\n",
    "        #store annotations per image based on imageID\n",
    "        \n",
    "        dic = {\n",
    "            'mask_path': mask_path,\n",
    "            'class': str(class_index),\n",
    "            'bbox': [xmin, xmax, ymin, ymax],\n",
    "            'box_id': bbox_id\n",
    "        }\n",
    "        \n",
    "        boxes[image_id].append(dic)\n",
    "        \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "backpack_boxes = get_all_annotations_for_each_image(annotations, backpack_mask_dir, 'Backpack', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contour Calculation\n",
    "\n",
    "`cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_TC89_L1)`\n",
    "\n",
    "`cv2.CHAIN_APPROX_TC89_L1` option provides only the dominant points of the contour instead of all the contour points when used other options.\n",
    "\n",
    "`contour.shape[1] = 1` ensures that we select only non hierarchical contours so that we get rid of multiple contours of the same instance.\n",
    "\n",
    "Read more about contour [here](https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#cv2.findContours) and [here](https://docs.opencv.org/3.4/d4/d73/tutorial_py_contours_begin.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def get_contour_pts(mask_path, width, height):\n",
    "    \n",
    "    # 0 is used to read image in gray scale mode\n",
    "    mask = cv2.imread(mask_path, 0)\n",
    "    \n",
    "    mask_img = cv2.resize(mask, (width, height), cv2.INTER_NEAREST)\n",
    "    ret, thresh = cv2.threshold(mask_img, 127, 255, 0)\n",
    "    orig_contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_L1)\n",
    "        \n",
    "    sorted_contours = []\n",
    "    cnt_area = []\n",
    "\n",
    "    for cnt in orig_contours:\n",
    "        if len(cnt.shape) == 3 and cnt.shape[1] == 1:\n",
    "            cnt_area.append(cv2.contourArea(cnt))\n",
    "            sorted_contours.append(cnt.reshape(-1).tolist())\n",
    "\n",
    "    contour = [p for p in sorted_contours if len(p) > 4]\n",
    "    area = [cnt_area[i] for i,p in enumerate(sorted_contours) if len(p) > 4]\n",
    "    \n",
    "    return (contour, area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets see how we used backpack train and validation set to generate `train.json` and `validation.json`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_COCO_formatted_json(boxes, mask_dir, image_dir, image_list, json_filepath):\n",
    "    annotations = []\n",
    "    images = []\n",
    "    categories = []\n",
    "\n",
    "    for img_id, img_meta_list in boxes.items():\n",
    "        \n",
    "        image_filename = '{}.jpg'.format(img_id)\n",
    "        \n",
    "        if image_filename not in image_list:\n",
    "            continue\n",
    "            \n",
    "        img_path = os.path.join(image_dir, image_filename)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        h, w, _ = img.shape\n",
    "\n",
    "        for index, img_meta in enumerate(img_meta_list):\n",
    "            class_id = img_meta[\"class\"]\n",
    "            xp1, xp2, yp1, yp2 = img_meta[\"bbox\"]\n",
    "            x1, x2, y1, y2 = float(xp1)*w, float(xp2)*w, float(yp1)*h, float(yp2)*h\n",
    "            \n",
    "            mask_path = os.path.join(mask_dir, img_meta[\"mask_path\"])\n",
    "\n",
    "            contour, area = get_contour_pts(mask_path, w, h)        \n",
    "\n",
    "            if not contour:\n",
    "                continue\n",
    "\n",
    "            bbox_coord = [int(x1), int(y1), int(x2) - int(x1), int(y2) - int(y1)]\n",
    "\n",
    "            annotations.append({\n",
    "                \"segmentation\": contour,\n",
    "                \"iscrowd\": 0,\n",
    "                \"image_id\": img_id,\n",
    "                \"bbox\": bbox_coord,\n",
    "                \"area\": area[0],\n",
    "                \"category_id\": class_id,\n",
    "                \"id\": img_meta[\"box_id\"]})\n",
    "\n",
    "\n",
    "        images.append({\"file_name\": img_id + \".jpg\",\n",
    "            \"height\": h,\n",
    "            \"width\": w,\n",
    "            \"id\": img_id})\n",
    "\n",
    "\n",
    "    categories.append({\"supercategory\": \"object\", \"id\": class_id, \"name\": \"backpack\"})\n",
    "\n",
    "\n",
    "    with open(json_filepath, \"w\") as f:\n",
    "        json.dump({\"images\": images, \"annotations\": annotations, \"categories\": categories}, f)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_json_path = os.path.join(data_root, 'train.json')\n",
    "validation_json_path = os.path.join(data_root, 'validation.json')\n",
    "\n",
    "# Get train.json in coco format\n",
    "\n",
    "generate_COCO_formatted_json(boxes=backpack_boxes, \n",
    "                             mask_dir=backpack_mask_dir, \n",
    "                             image_dir=backpack_image_dir, \n",
    "                             image_list=train_images, \n",
    "                             json_filepath=train_json_path)\n",
    "\n",
    "# Get validation.json in coco format\n",
    "\n",
    "generate_COCO_formatted_json(boxes=backpack_boxes, \n",
    "                             mask_dir=backpack_mask_dir, \n",
    "                             image_dir=backpack_image_dir, \n",
    "                             image_list=val_images, \n",
    "                             json_filepath=validation_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:green\"> 4. Train using Detectron2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'detectron2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-4e6d721767b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msetup_logger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msetup_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# import some common libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'detectron2'"
     ]
    }
   ],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "\n",
    "# model_zoo has a lots of pre-trained model\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "# DefaultTrainer is a class for training instance segmentation model and inference\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "\n",
    "# detectron2 has its configuration format\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "# detectron2 has implemented Visualizer of object detection\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "\n",
    "# from DatasetCatalog, detectron2 gets dataset and from MetadatCatalog it gets metadata of the dataset\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "# Registers COCO format datasets\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "# COCOEvaluator based on COCO evaluation metric, inference_on_dataset is used for evaluation for a given metric\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "\n",
    "# build_detection_test_loader, used to create test loader for evaluation\n",
    "from detectron2.data import build_detection_test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4.1. Register Dataset</font>\n",
    "\n",
    "As the dataset is already in COCO format, we can use the following method\n",
    "\n",
    "`register_coco_instances(name, metadata, json_file, image_root)`\n",
    "\n",
    "This method internally calls **load_coco_json** method, registers dataset and also adds metadata.\n",
    "\n",
    "`load_coco_json(json_file, image_root, dataset_name=None, extra_annotation_keys=None)` takes a json file with COCO's instances annotation format. Currently supports instance detection, instance segmentation, and person keypoints annotations.returns the list of detectron2 standard dataset dicts format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_name = 'backpack_train'\n",
    "val_data_name = 'backpack_val'\n",
    "\n",
    "register_coco_instances(train_data_name, {}, train_json_path, backpack_image_dir)\n",
    "register_coco_instances(val_data_name, {}, validation_json_path, backpack_image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4.2. Visualize Training Data</font>\n",
    "\n",
    "Lets visualize some of the loaded training samples using Visualizer functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "backpack_train_metadata = MetadataCatalog.get(train_data_name)\n",
    "\n",
    "dataset_dicts = DatasetCatalog.get(name=train_data_name)\n",
    "\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=backpack_train_metadata, scale=0.5)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    plt.figure(figsize = (12, 12))\n",
    "    plt.imshow(vis.get_image())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4.3. Set Config and Import model files</font>\n",
    "\n",
    "Get default configuration of detectron2 and update parameters according to our requirements.\n",
    "\n",
    "Get more details of the configuration <a  href=\"https://detectron2.readthedocs.io/modules/config.html#\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize with default configuration\n",
    "cfg = get_cfg()\n",
    "\n",
    "# update configuration with MaskRCNN configuration\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "# Let's replace the detectron2 default train dataset with our train dataset.\n",
    "cfg.DATASETS.TRAIN = (train_data_name,)\n",
    "\n",
    "# No metric implemented for the test dataset, we will have to update cfg.DATASET.TEST with empty tuple\n",
    "cfg.DATASETS.TEST = ()\n",
    "\n",
    "# data loader configuration\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "\n",
    "# Update model URL in detectron2 config file\n",
    "# Let training initialize from model zoo\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  \n",
    "\n",
    "# batch size\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "\n",
    "# choose a good learning rate\n",
    "cfg.SOLVER.BASE_LR = 0.0005\n",
    "\n",
    "# We need to specify the number of iteration for training in detectron2, not the number of epochs.\n",
    "cfg.SOLVER.MAX_ITER = 500\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "\n",
    "# number of output class\n",
    "# we have only one class that is Backpack\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4.4. Training</font>\n",
    "\n",
    "Now, let's fine-tune a coco-pretrained Mask RCNN R50 FPN model on the backpack dataset.\n",
    "\n",
    "detectron2.engine.defaults.DefaultTrainer(cfg)\n",
    "A trainer with default training logic. Compared to SimpleTrainer, it contains the following logic in addition:\n",
    "\n",
    "- Create model, optimizer, scheduler, dataloader from the given config.\n",
    "- Load a checkpoint or cfg.MODEL.WEIGHTS, if exists, when resume_or_load is called.\n",
    "- Register a few common hooks\n",
    "\n",
    "Get more details <a href=\"https://detectron2.readthedocs.io/modules/engine.html#detectron2.engine.defaults.DefaultTrainer\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update create ouptput directory \n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the training, let's set up the TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "# Create a trainer instance with the configuration.\n",
    "trainer = DefaultTrainer(cfg) \n",
    "\n",
    "# if resume=False, because we don't have trained model yet. It will download model from model url and load it\n",
    "trainer.resume_or_load(resume=False)\n",
    "\n",
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:green\">5. Inference using the trained model</font>\n",
    "\n",
    "Now, let's run inference with the trained model on the backpack test dataset. First, let's create a predictor using the model we just trained.\n",
    "\n",
    "detectron2.engine.defaults.DefaultPredictor(cfg)\n",
    "Create a simple end-to-end predictor with the given config that runs on single device for a single input image. Compared to using the model directly, this class does the following additions:\n",
    "\n",
    "- Load checkpoint from cfg.MODEL.WEIGHTS.\n",
    "- Always take BGR image as the input and apply conversion defined by cfg.INPUT.FORMAT.\n",
    "- Apply resizing defined by cfg.INPUT.{MIN,MAX}_SIZE_TEST.\n",
    "- Take one input image and produce a single output, instead of a batch.\n",
    "\n",
    "Get more details <a href=\"https://detectron2.readthedocs.io/modules/engine.html#detectron2.engine.defaults.DefaultPredictor\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on our fine-tuned model\n",
    "\n",
    "# By default detectron2 save the model with name model_final.pth\n",
    "# update the model path in configuration that will be used to load the model\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "# set the testing threshold for this model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9\n",
    "\n",
    "\n",
    "cfg.DATASETS.TEST = (val_data_name,)\n",
    "\n",
    "backpack_test_metadata = MetadataCatalog.get(val_data_name)\n",
    "\n",
    "# create a predictor instance with the configuration (it has our fine-tuned model)\n",
    "# this predictor does prdiction on a single image\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's have a look on prediction\n",
    "test_dataset_dicts = DatasetCatalog.get(val_data_name)\n",
    "\n",
    "for d in random.sample(test_dataset_dicts, 3):\n",
    "    print(d[\"file_name\"])\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=backpack_test_metadata, scale=0.5)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    plt.figure(figsize = (12, 12))\n",
    "    plt.imshow(vis.get_image())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:green\">6. Inference on Video</font>\n",
    "\n",
    "Lets run inference on a small surveillance video using our trained model.\n",
    "\n",
    "**[Download the input video.](https://www.dropbox.com/s/0yx6lyxxg37cq8i/videoplayback_short.mp4?dl=0)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_on_video(video_path, out_video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cnt = 0\n",
    "\n",
    "    output_frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, im = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if cnt%5 == 0:\n",
    "            outputs = predictor(im)\n",
    "            v = Visualizer(im[:, :, ::-1],\n",
    "                           metadata=backpack_test_metadata, \n",
    "                           scale=0.8, \n",
    "#                            instance_mode=ColorMode.IMAGE_BW\n",
    "                          )\n",
    "            v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "            output_frames.append(v.get_image()[:, :, ::-1])\n",
    "\n",
    "        cnt = cnt + 1\n",
    "\n",
    "    height, width, _ = output_frames[0].shape\n",
    "    size = (width,height)\n",
    "    out = cv2.VideoWriter(out_video_path, cv2.VideoWriter_fourcc(*'mp4v'), 10, size)\n",
    "\n",
    "    for i in range(len(output_frames)):\n",
    "        out.write(output_frames[i])\n",
    "\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output video is stored as out.mp4 with backpack segmentations\n",
    "\n",
    "inference_on_video('videoplayback_short.mp4', 'videoplayback_short_out.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
