{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Subclassing_models import SubclassSequential, SubclassModel, SubclassFunctionalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubclassSequential(\n",
      "  (model): Sequential(\n",
      "    (hidden_linear): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (hidden_activation): Tanh()\n",
      "    (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_neurons = 8\n",
    "seq_model = SubclassSequential(n_neurons)\n",
    "print(seq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubclassModel(\n",
      "  (hidden_linear): Linear(in_features=1, out_features=13, bias=True)\n",
      "  (hidden_activation): Tanh()\n",
      "  (output_linear): Linear(in_features=13, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "subclass_model = SubclassModel()\n",
    "print(subclass_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubclassFunctionalModel(\n",
      "  (hidden_linear): Linear(in_features=1, out_features=14, bias=True)\n",
      "  (output_linear): Linear(in_features=14, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "subclassFunctional_model = SubclassFunctionalModel()\n",
    "print(subclassFunctional_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c).unsqueeze(1) # <1>\n",
    "t_u = torch.tensor(t_u).unsqueeze(1) # <1>\n",
    "\n",
    "\n",
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "\n",
    "t_u_train = t_u[train_indices]\n",
    "t_c_train = t_c[train_indices]\n",
    "\n",
    "t_u_val = t_u[val_indices]\n",
    "t_c_val = t_c[val_indices]\n",
    "\n",
    "t_un_train = 0.1 * t_u_train\n",
    "t_un_val = 0.1 * t_u_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs:int , optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val):\n",
    "    \n",
    "    # _Start: training loop for n_epochs\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        # _Start: for training set \n",
    "        t_p_train = model(t_u_train) # <1>\n",
    "        loss_train = loss_fn(t_p_train, t_c_train)\n",
    "        # _End: for training set \n",
    "        \n",
    "        # _Start: for validation set \n",
    "        with torch.no_grad():\n",
    "            t_p_val = model(t_u_val) # <1>\n",
    "            loss_val = loss_fn(t_p_val, t_c_val)\n",
    "            assert loss_val.requires_grad == False\n",
    "        # _End: for validation set \n",
    "            \n",
    "         # _Start: backpropagation and update \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward() # <2>\n",
    "        optimizer.step()\n",
    "        # _End: backpropagation and update \n",
    "\n",
    "        if epoch == 1 or epoch % 1000 == 0:\n",
    "            print('Epoch {}, Training loss {}, Validation loss {}'.format(\n",
    "                epoch, float(loss_train), float(loss_val)))\n",
    "    # _End: training loop for n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 1.320799708366394, Validation loss 7.433529853820801\n",
      "Epoch 1000, Training loss 1.3198957443237305, Validation loss 7.438112735748291\n",
      "Epoch 2000, Training loss 1.3191473484039307, Validation loss 7.431526184082031\n",
      "Epoch 3000, Training loss 1.3184970617294312, Validation loss 7.420357704162598\n",
      "Epoch 4000, Training loss 1.3179190158843994, Validation loss 7.407679557800293\n",
      "Epoch 5000, Training loss 1.3173913955688477, Validation loss 7.394316673278809\n",
      "output tensor([[8.3193],\n",
      "        [3.5675]], grad_fn=<AddmmBackward>)\n",
      "answer tensor([[6.0000],\n",
      "        [0.5000]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SubclassSequential' object has no attribute 'hidden_linear'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7e604e7d892e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_un_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'answer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_c_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hidden'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_linear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    592\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 594\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SubclassSequential' object has no attribute 'hidden_linear'"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD( seq_model.parameters(),  lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 5000, \n",
    "    optimizer = optimizer,\n",
    "    model = seq_model,\n",
    "    loss_fn = loss_fn,\n",
    "    t_u_train = t_un_train,\n",
    "    t_u_val = t_un_val, \n",
    "    t_c_train = t_c_train,\n",
    "    t_c_val = t_c_val)\n",
    "\n",
    "print('output: ', seq_model(t_un_val))\n",
    "print('answer: ', t_c_val)\n",
    "print('hidden: ', seq_model.hidden_linear.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env] *",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
